{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tweepy\n",
    "sys.path.insert(1, '../../')\n",
    "import keys\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import string\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from unidecode import unidecode\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api conf\n",
    "auth = tweepy.OAuthHandler(keys.cos_pub, keys.cos_sec)\n",
    "auth.set_access_token(keys.as_pub, keys.as_sec)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "data_folder = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting data\n",
    "#TODO criar um gerador de termos de pesquisa\n",
    "search_terms = [\"notebook\", 'celular', 'câmera', 'iphone', 'câmera', 'smartwatsh', 'memória', 'ventilador', 'aplicativo']\n",
    "def twitter_search(terms):\n",
    "    tmp_search_result = []\n",
    " \n",
    "    for e in search_terms:\n",
    "        partial_result = [];\n",
    "        api_result = api.search(q=e, tweet_mode='extended', count=100, lang='pt', retweeted=False, reply=False)\n",
    "\n",
    "        for prop in api_result:\n",
    "            partial_result.append(prop._json['full_text'])\n",
    "        tmp_search_result.extend(partial_result)\n",
    "        \n",
    "    return tmp_search_result\n",
    "search_result = twitter_search(search_terms)\n",
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving data in a txt file\n",
    "\n",
    "data_path = '{0}{1}/'.format(data_folder, datetime.now(tz=None))\n",
    "graphics_path = data_path +'graphics/' \n",
    "separator = ' \\n || \\n '\n",
    "\n",
    "os.makedirs(graphics_path)\n",
    "\n",
    "#raw file\n",
    "with open(data_path+'raw.txt', 'wb') as file:\n",
    "    for e in search_result:\n",
    "        entry = '{0}{1}'.format(e.splitlines(), separator) #tweet separator\n",
    "        file.write(entry.encode())\n",
    "    file.close()\n",
    "    \n",
    "#search terms\n",
    "with open(data_path+'search_terms.txt', 'wb') as file:\n",
    "    entry = ' '.join(search_terms)\n",
    "    file.write(entry.encode())\n",
    "    file.close()\n",
    "    \n",
    "with open(data_path+'raw_separator.txt', 'wb') as file:\n",
    "    file.write(separator.encode())\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# juntando todas as palavras e removendo \\n\n",
    "group_text = ''.join(''.join(search_result).splitlines())\n",
    "group_text = unidecode(group_text)\n",
    "del(search_result)\n",
    "group_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtrando\n",
    "stop_words = nltk.corpus.stopwords.words('portuguese')\n",
    "stop_words.extend(['', 'rt']) #tirando espaços vazios e marcas de retweet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "words_list = []\n",
    "words_list = group_text.split()\n",
    "    \n",
    "words_list = [e.strip().strip(string.punctuation).lower() for e in words_list]\n",
    "words_list = [e for e in words_list if e not in stop_words or \"https\" not in e or e[0] !=  '@'] #tirando stop words links e menções  \n",
    "\n",
    "#removendo palavras onde houve pontuação,m as o espaçamento não ocorreu\n",
    "for i, word in enumerate(words_list):\n",
    "    tmp_sentense = ' '.join([e for e in word if e.isalnum()])\n",
    "    try:\n",
    "        tmp_sentense = tmp_sentense.split(' ')\n",
    "        for j, e in enumerate(tmp_sentense):\n",
    "            words_list.insert(j+i, e)\n",
    "    except:\n",
    "        False\n",
    "        \n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "del(group_text) #liberando memória\n",
    "\n",
    "words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#termos mais frequentes\n",
    "frequence_list= []\n",
    "frequence_list = Counter(words_list)\n",
    "\n",
    "frequence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = ' '.join(words_list)\n",
    "\n",
    "\n",
    "word_cloud = WordCloud(max_font_size=50).generate(filtered_words)\n",
    "\n",
    "with open(data_path+'filtered.txt', \"wb\") as file:\n",
    "    file.write(filtered_words.encode())\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "ax.imshow(word_cloud, interpolation='nearest')\n",
    "plt.axis(label= 'Nearest')\n",
    "plt.tight_layout()\n",
    "plt.savefig(graphics_path+'w_c_near.png', dpi=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algoritmo que mostra vizinhos\n",
    "expression = \"ventilador\"\n",
    "\n",
    "positions = [] #localização onde a plavra se encontra\n",
    "for i, e in enumerate(words_list):\n",
    "    if expression == e:\n",
    "        positions.append(i)\n",
    "\n",
    "print(len(positions))\n",
    "print(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = 2\n",
    "for e in positions[0:30]:\n",
    "    print(words_list[e-neighbors : e+neighbors+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
